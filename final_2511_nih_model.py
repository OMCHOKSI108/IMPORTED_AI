# -*- coding: utf-8 -*-
"""final_2511_nih_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CqZxJT4gOH3SdEZlg_hcjbMhakL1RwvO
"""
#Libraries
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer
import json
import pickle
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD
import random
nltk.download('punkt')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

words = []
classes = []
documents = []
ignore_words = ['?', '!']


#Preprocess of the data
data_file = open('Final_2511.json').read()
intents = json.loads(data_file)

for intent in intents['intents']:
    for pattern in intent['patterns']:
        w = nltk.word_tokenize(pattern)
        words.extend(w)
        documents.append((w, intent['tag']))
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))
classes = sorted(list(set(classes)))
print(len(documents), "documents")
print(len(classes), "classes", classes)
print(len(words), "unique lemmatized words", words)
pickle.dump(words, open('final_texts.pkl', 'wb'))
pickle.dump(classes, open('final_labels.pkl', 'wb'))

#Training  data
training = []
output_empty = [0] * len(classes) #Creating a list of number of classes for one-hot encoding 

for doc in documents:
    bag = []
    pattern_words = doc[0]
    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)

    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1 #binary  vector to represent the classes for one-hot encoding 

    training.append([bag, output_row])

random.shuffle(training)  # avoiding biases
training = np.array(training)
train_x = list(training[:, 0])
train_y = list(training[:, 1])
print("Training data created")
# The input data consists of the bag-of-words representations, and the output data consists of the corresponding one-hot encoded labels.
print("Shape of train_x:", np.array(train_x).shape)
print("Shape of train_y:", np.array(train_y).shape)

from keras.optimizers import Adam  #adam is used to update the weights of the NN 

#feedforward network with 2 hidden layer
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5)) #prevents all neurons in a layer from synchronously optimizing their weights
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))  #used for multi-class classification problems. Softmax converts the raw output into probabilities for each class
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
hist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=8, verbose=1)
model.save('finalmodel.h5', hist)
print("model created")

