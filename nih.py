# -*- coding: utf-8 -*-
"""NIH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1trVdyeSZpHXvXJvXCxl5zkNIhZ76pXLl
"""

# -*- coding: utf-8 -*-
"""final_2511_nih_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CqZxJT4gOH3SdEZlg_hcjbMhakL1RwvO
"""
#Libraries
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer
import json
import pickle
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD
import random
nltk.download('punkt')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

words = []
classes = []
documents = []
ignore_words = ['?', '!']


#Preprocess of the data
data_file = open('Final_11.json').read()
intents = json.loads(data_file)

for intent in intents['intents']:
    for pattern in intent['patterns']:
        w = nltk.word_tokenize(pattern)
        words.extend(w)
        documents.append((w, intent['tag']))
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))
classes = sorted(list(set(classes)))
print(len(documents), "documents")
print(len(classes), "classes", classes)
print(len(words), "unique lemmatized words", words)
pickle.dump(words, open('final_texts.pkl', 'wb'))
pickle.dump(classes, open('final_labels.pkl', 'wb'))

#Training  data
training = []
output_empty = [0] * len(classes) #Creating a list of number of classes for one-hot encoding

for doc in documents:
    bag = []
    pattern_words = doc[0]
    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)

    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1 #binary  vector to represent the classes for one-hot encoding

    training.append([bag, output_row])

random.shuffle(training)  # avoiding biases
training = np.array(training)
train_x = list(training[:, 0])
train_y = list(training[:, 1])
print("Training data created")
# The input data consists of the bag-of-words representations, and the output data consists of the corresponding one-hot encoded labels.
print("Shape of train_x:", np.array(train_x).shape)
print("Shape of train_y:", np.array(train_y).shape)

from keras.optimizers import Adam  #adam is used to update the weights of the NN

#feedforward network with 2 hidden layer
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5)) #prevents all neurons in a layer from synchronously optimizing their weights
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))  #used for multi-class classification problems. Softmax converts the raw output into probabilities for each class
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
hist = model.fit(np.array(train_x), np.array(train_y), epochs=500, batch_size=8, verbose=1)
model.save('finalmodel.h5', hist)
print("model created")

import nltk
from nltk.stem import WordNetLemmatizer
import pickle
import numpy as np
from keras.models import load_model
from nltk.tokenize import sent_tokenize
import json
import tensorflow as tf
import random
import os

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('popular')

lemmatizer = WordNetLemmatizer()

model = load_model('finalmodel.h5')
intents = json.loads(open('Final_11.json').read())
words = pickle.load(open('final_texts.pkl', 'rb'))
classes = pickle.load(open('final_labels.pkl', 'rb'))

def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print("found in bag: %s" % w)
    return np.array(bag)

def predict_class(sentence, model):
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = [{"intent": classes[r[0]], "probability": str(r[1])} for r in results]
    return return_list

def get_response(ints, intents_json):
    tag = ints[0]['intent']
    list_of_intents = intents_json['intents']
    for i in list_of_intents:
        if i['tag'] == tag:
            responses = i['responses']
            response = random.choice(responses)
            sentences = sent_tokenize(response)
            sentence = random.choice(sentences)
            break
    return sentence

def get_all_related_questions(tag, intents_json):
    list_of_intents = intents_json['intents']
    related_questions = []

    for i in list_of_intents:
        if i['tag'] == tag:
            related_questions.extend(i['patterns'])

    return related_questions

def update_dataset(user_input, bot_response, dataset_path='Final_11.json'):
    intents = json.loads(open(dataset_path).read())
    list_of_intents = intents['intents']

    existing_intent = next((intent for intent in list_of_intents if intent['tag'] == user_input), None)

    if existing_intent:
        existing_intent['patterns'].append(user_input)
        existing_intent['responses'].append(bot_response)
    else:
        new_intent = {
            'tag': user_input,
            'patterns': [user_input],
            'responses': [bot_response]
        }
        list_of_intents.append(new_intent)

    with open(dataset_path, 'w') as json_file:
        json.dump(intents, json_file, indent=4)

def chat():
    while True:
        print("Start")
        print("Chatbot Response: Hello! How can I assist you today?")

        conversation = []

        while True:
            message = input("You: ")

            if message.lower() == 'exit':
                print("Chatbot Response: Have a nice day!")
                break

            # Check if the question has been asked before
            previous_answer = next((response for input_msg, response in reversed(conversation) if input_msg == message), None)

            if previous_answer:
                print("Chatbot Response:", previous_answer)
            else:
                ints = predict_class(message, model)
                res = get_response(ints, intents)
                print("Chatbot Response:", res)

                user_feedback = input("Was the response helpful? (yes/no): ").lower()
                conversation.append((message, res))

                if user_feedback == 'no':
                    related_questions = get_all_related_questions(ints[0]['intent'], intents)

                    # Provide options for related questions
                    print("These are related questions:")
                    for idx, question in enumerate(related_questions, start=1):
                        print(f"{idx}. {question}")

                    try:
                        selected_option = int(input("Select a question number to get a response (or enter 0 to continue): "))

                        if 0 < selected_option <= len(related_questions):
                            selected_question = related_questions[selected_option - 1]
                            print(f"You selected: {selected_question}")
                            res = get_response(predict_class(selected_question, model), intents)
                            print("Chatbot Response:", res)
                            conversation.append((selected_question, res))
                        elif selected_option == 0:
                            pass
                        else:
                            print("Invalid option. Please try again.")
                    except ValueError:
                        print("Invalid input. Please enter a number.")

if __name__ == "__main__":
    chat()

